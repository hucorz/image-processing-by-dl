{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 大致流程\n","\n","- 把训练数据上传至 google drive，如 flower_photos.tgz\n","\n","- 挂载到 google drive\n","\n","- 数据解压缩\n","\n","- 数据集预处理（如果需要）\n","\n","- model 的定义\n","\n","- model 的训练：对于运行时必要的参数需要手动添加\n","  \n","```python\n","config = parser.parse_args(args=[\"--img_path\", \"./flower_data/\", \"--vgg_version\", \"vgg11\"])\n","```\n","\n","以下是一个 vgg 的 ipynb 的例子"]},{"cell_type":"markdown","metadata":{"id":"vRmNIKwP4p0f"},"source":["## 上传所需数据到 google drive\n","\n","把 flower_photos.tgz 上传到 google drive"]},{"cell_type":"markdown","metadata":{"id":"bLVYlhNe3mSZ"},"source":["## Data preprocessing\n","\n","挂载到 google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16326,"status":"ok","timestamp":1650441487341,"user":{"displayName":"97 znk","userId":"15592115286717311381"},"user_tz":-480},"id":"X5kimpic3TPz","outputId":"11323fdf-6b84-4715-a878-7a4c9d7a22c0"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5373,"status":"ok","timestamp":1650441494471,"user":{"displayName":"97 znk","userId":"15592115286717311381"},"user_tz":-480},"id":"RAiDt2iv3yOC","outputId":"47ebac0b-39e5-4661-eec7-36d8a1e2266c"},"outputs":[],"source":["!mkdir flower_data\n","!tar -xvf ./drive/MyDrive/image-processing-by-dl/flower_photos.tgz -C flower_data"]},{"cell_type":"markdown","metadata":{"id":"iOl5_3H35Efm"},"source":["## Data split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4516,"status":"ok","timestamp":1650441505240,"user":{"displayName":"97 znk","userId":"15592115286717311381"},"user_tz":-480},"id":"UsT7MZYi5GSI","outputId":"8048e8a5-3f00-4364-fb7b-acb1c3b1a74f"},"outputs":[],"source":["import os\n","from shutil import copy, rmtree\n","import random\n","\n","def mk_file(file_path: str):\n","    if os.path.exists(file_path):\n","        # 如果文件夹存在，则先删除原文件夹在重新创建\n","        rmtree(file_path)\n","    os.makedirs(file_path)\n","\n","\n","def main():\n","    # 保证随机可复现\n","    random.seed(0)\n","\n","    # 将数据集中10%的数据划分到验证集中\n","    split_rate = 0.1\n","\n","    # 指向你解压后的flower_photos文件夹\n","    cwd = os.getcwd()\n","    data_root = os.path.join(cwd, \"flower_data\")\n","    origin_flower_path = os.path.join(data_root, \"flower_photos\")\n","    assert os.path.exists(origin_flower_path), \"path '{}' does not exist.\".format(origin_flower_path)\n","\n","    flower_class = [cla for cla in os.listdir(origin_flower_path)\n","                    if os.path.isdir(os.path.join(origin_flower_path, cla))]\n","\n","    # 建立保存训练集的文件夹\n","    train_root = os.path.join(data_root, \"train\")\n","    mk_file(train_root)\n","    for cla in flower_class:\n","        # 建立每个类别对应的文件夹\n","        mk_file(os.path.join(train_root, cla))\n","\n","    # 建立保存验证集的文件夹\n","    val_root = os.path.join(data_root, \"val\")\n","    mk_file(val_root)\n","    for cla in flower_class:\n","        # 建立每个类别对应的文件夹\n","        mk_file(os.path.join(val_root, cla))\n","\n","    for cla in flower_class:\n","        cla_path = os.path.join(origin_flower_path, cla)\n","        images = os.listdir(cla_path)\n","        num = len(images)\n","        # 随机采样验证集的索引\n","        eval_index = random.sample(images, k=int(num*split_rate))\n","        for index, image in enumerate(images):\n","            if image in eval_index:\n","                # 将分配至验证集中的文件复制到相应目录\n","                image_path = os.path.join(cla_path, image)\n","                new_path = os.path.join(val_root, cla)\n","                copy(image_path, new_path)\n","            else:\n","                # 将分配至训练集中的文件复制到相应目录\n","                image_path = os.path.join(cla_path, image)\n","                new_path = os.path.join(train_root, cla)\n","                copy(image_path, new_path)\n","            print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\n","        print()\n","\n","    print(\"processing done!\")\n","\n","main()"]},{"cell_type":"markdown","metadata":{"id":"aPwhGKSs7Sp4"},"source":["## Define model"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6367,"status":"ok","timestamp":1650441520027,"user":{"displayName":"97 znk","userId":"15592115286717311381"},"user_tz":-480},"id":"0wCFWSpn7VPy"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_version, num_classes=1000):\n","        super().__init__()\n","        self.enc = get_vgg_enc(vgg_version)    # 编码器 (特征提取)\n","        self.head = nn.Sequential( # 预测头\n","            nn.Linear(512*7*7, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        feats = self.enc(img) # 5 个 maxpool, (3x224x224) -> (512x7x7)\n","        feats = torch.flatten(feats, start_dim=1)\n","        output = self.head(feats)\n","        return output\n","\n","def get_vgg_enc(vgg_version):\n","    assert vgg_version in cfgs.keys(), \"vgg version not found\"\n","\n","    layers = []\n","    in_channels = 3\n","    for v in cfgs[vgg_version]:\n","        if v == 'M':\n","            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n","        else:\n","            layers.append(nn.Conv2d(in_channels, v, kernel_size=3, padding=1, stride=1))\n","            layers.append(nn.ReLU())\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","        \n","cfgs = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}"]},{"cell_type":"markdown","metadata":{"id":"DjBL_vFO7hVp"},"source":["## Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sn4ZMZxq7jqS","outputId":"21f523c3-791e-4c6d-fae2-a01027acbc4d"},"outputs":[],"source":["import argparse\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","from torchvision import transforms, datasets, utils\n","\n","import matplotlib.pyplot as plt\n","\n","def get_config():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--img_path\", type=str, help=\"path of image to train, if None\", default=None, required=True)\n","    parser.add_argument(\"--vgg_version\", type=str, help=\"vgg version, optional: vgg11, vgg13, vgg16, vgg19\", default=None, required=True)\n","    parser.add_argument(\"--output_path\", type=str, help=\"output file's saving path\", default=\"./output\", required=False)\n","    parser.add_argument(\"--lr\", type=float, help=\"learning rate\", default=0.0002, required=False)\n","    parser.add_argument(\"--epoch\", type=int, help=\"epoch\", default=20, required=False)\n","    parser.add_argument(\"--batch_size\", type=int, help=\"batch size\", default=32, required=False)\n","\n","    config = parser.parse_args(args=[\"--img_path\", \"./flower_data/\", \"--vgg_version\", \"vgg11\"])\n","    return config\n","\n","def main(config):\n","    print(config)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    transform = {\n","        \"train\": transforms.Compose([\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ]),\n","        \"val\": transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    }\n","\n","    nw = min([os.cpu_count(), 8, config.batch_size if config.batch_size > 1 else 0])\n","\n","    assert config.img_path, \"img_path is needed\"\n","\n","    train_root = os.path.join(config.img_path, \"train\")\n","    val_root = os.path.join(config.img_path, \"val\")\n","    train_set = datasets.ImageFolder(root=train_root, transform=transform[\"train\"])\n","    train_loader = DataLoader(train_set, shuffle=True, batch_size=config.batch_size, num_workers=nw)\n","    val_set = datasets.ImageFolder(root=val_root, transform=transform[\"val\"])\n","    val_loader = DataLoader(val_set, shuffle=False, batch_size=config.batch_size, num_workers=nw)\n","    print(f\"length of train set: {len(train_set)}\")\n","    print(f\"length of val set: {len(val_set)}\")\n","\n","    class2idx = train_set.class_to_idx\n","    print(class2idx)\n","    idx2class = dict((idx, cla) for cla, idx in class2idx.items())\n","\n","    model = VGG(config.vgg_version, len(class2idx.items())).to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n","\n","    train_loss_record = []    # train loss record each 100 steps\n","    train_acc_record = []     # train acc record\n","    val_loss_record = []    # val loss record\n","    val_acc_record = []     # val acc record\n","    best_val_acc = 0.0\n","\n","\n","    for epoch in range(config.epoch):\n","        running_loss, running_acc = 0.0, 0.0\n","        model.train()\n","        pbar = tqdm(train_loader)\n","        for data in pbar:\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            acc = (outputs.argmax(dim=-1) == labels).float().sum()\n","\n","            running_loss += loss.item()\n","            running_acc += acc.item()\n","            # pbar.desc = f\"[{epoch+1} / {config.epoch}] loss: {loss}\"\n","\n","        model.eval()\n","        val_loss, val_acc = 0.0, 0.0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                \n","                loss = criterion(outputs, labels)\n","                acc = (outputs.argmax(dim=-1) == labels).float().sum()\n","\n","                val_loss += loss.item()\n","                val_acc += acc.item()\n","\n","            \n","            val_loss_record.append(val_loss)\n","            val_acc_record.append(val_acc / len(val_set))\n","            train_loss_record.append(running_loss)\n","            train_acc_record.append(running_acc / len(train_set))\n","\n","            if (val_acc_record[-1] > best_val_acc):\n","                torch.save(model.state_dict(), os.path.join(config.output_path, f\"VGG_checkpoint.pth\"))\n","                best_val_acc = val_acc_record[-1]\n","            print(f\"[epoch:{epoch+1:03d}/{config.epoch:03d}] train loss:{train_loss_record[-1]:.4f}, train acc:{train_acc_record[-1]:.4f} | val loss:{val_loss_record[-1]:.4f} val acc:{val_acc_record[-1]:.4f}\")\n","\n","    np.save(os.path.join(config.output_path, \"train_loss_record.npy\"), train_loss_record)\n","    np.save(os.path.join(config.output_path, \"train_acc_record.npy\"), train_acc_record)\n","    np.save(os.path.join(config.output_path, \"val_loss_record.npy\"), val_loss_record)\n","    np.save(os.path.join(config.output_path, \"val_acc_record.npy\"), val_acc_record)\n","    plt.figure()\n","    plt.subplot(221);\n","    plt.plot(train_loss_record);plt.title(\"train loss record\");\n","    plt.subplot(222);\n","    plt.plot(train_acc_record);plt.title(\"train acc record\");\n","    plt.subplot(223);\n","    plt.plot(val_loss_record);plt.title(\"val loss record\");\n","    plt.subplot(224);\n","    plt.plot(val_acc_record);plt.title(\"val acc record\");\n","    plt.savefig(os.path.join(config.output_path, \"result.png\"))\n","\n","\n","config = get_config()\n","\n","if config.output_path == \"./output\" and \"output\" not in os.listdir(\"./\"):\n","    os.mkdir(\"./output\")\n","\n","myseed = 42069  # set a random seed for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(myseed)\n","torch.manual_seed(myseed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(myseed)\n","    torch.cuda.manual_seed_all(myseed)\n","\n","main(config)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM0GPne3Gpwv4WIani95Tau","collapsed_sections":[],"name":"VGG_test.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
